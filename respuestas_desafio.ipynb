{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para este desdafío se utiliza Python 3.7.3 y las siguientes librerías:\n",
    "- numpy==1.19.1\n",
    "- pandas==1.2.4\n",
    "- matplotlib==3.3.1 \n",
    "- scikit-learn==0.23.2\n",
    "- statsmodels=0.12.0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos librerías estándar que usaremos en esta primera parte del desafío\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Datos:\n",
    "    \n",
    "Cargamos los datos usando la librería Pandas y visualizamos para hacer una rápida inspección visual.    \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos datos de precipitaciones\n",
    "precip = pd.read_csv('precipitaciones.csv')\n",
    "\n",
    "# Mostramos los datos de precipitaciones para una rápida inspección visual\n",
    "precip.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos datos del banco central\n",
    "banco = pd.read_csv('banco_central.csv')\n",
    "\n",
    "# Mostramos los datos del banco central para una rápida inspección visual\n",
    "banco.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Análisis de los datos\n",
    "\n",
    "Al cargar los datos, se ve que hay errores e inconsistencias en los formatos usados. Por ejemplo, hay ambigüedades respecto a cómo se deben interpretar los puntos en datos numéricos.\n",
    "\n",
    "En los datos del banco central hay datos duplicados, valores que no numéricos (hay unas celdas con valor 'a') y muchos datos faltantes, que son rellenados con 'Nan'. \n",
    "\n",
    "Adicionalmente, los las fechas de los datos del banco central incluyen horas, minutos y segundo (aunque no es información relevante en este caso). Los datos de precipicationes solo incluyen la fecha.\n",
    "\n",
    "Para cada una de las tablas de datos estandarizamos el uso de la fecha, eliminamos columnas vacías, removemos duplicados, convertimos valores a numéricos y eliminamos outliers (consideramos outliers datos bajo el cuantil 0.01 y sobre a 0.99). Aunque no haya outliers, se eliminará el 2% de los datos de los extremos de la distribución lo que no debería impactar mucho el análisis posterior. Un análisis más detallado se podría hacer para solamente aplicar este critero a los datos más 'ruidosos', pero tomaría más tiempo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertimos los datos de la columna 'date' a datetime\n",
    "precip['date'] = pd.to_datetime(precip['date'], errors='coerce', yearfirst=True).dt.date\n",
    "# Seteamos el datetime como el index de la tabla\n",
    "precip = precip.set_index('date')\n",
    "# Eliminamos filas con index inválido (Nan). Al aprecer en esta tabla no hay, pero es mejor asegurarse. \n",
    "precip = precip[precip.index.notnull()]\n",
    "# Si hay datos duplicados, los eliminamos. Al aprecer en esta tabla no hay, pero es mejor asegurarse. \n",
    "precip = precip.drop_duplicates()\n",
    "# Por último convertimos los valores de las céldas a numérico. \n",
    "# Se puede apreciar que los puntos son usados para indicar miles y no decimales, lo que no es interpretado por Pandas.\n",
    "# Simplemente removemos los puntos y convertimos a numérico. Usamos 'coerce' para forzar inválidos a 'Nan'\n",
    "for col_name in precip:\n",
    "    precip[col_name] = [x.replace('.','') if isinstance(x, str) else x for x in precip[col_name] ]    \n",
    "    precip[col_name] = pd.to_numeric(precip[col_name] ,errors='coerce')\n",
    "# Removemos posibles outliers, es decir datos bajo el cuantil 0.01 y sobre 0.99\n",
    "precip = precip[(precip < precip.quantile(0.99)) & (precip>  precip.quantile(0.01))]\n",
    "# Ordenamos por fecha\n",
    "precip = precip.sort_index()\n",
    "\n",
    "# Nuevamente mostramos los datos para verificar que la tabla está con el formato deseado\n",
    "precip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertimos los datos de la columna 'Periodo' a datetime y eliminamos la hora\n",
    "banco['Periodo'] = pd.to_datetime(banco['Periodo'], errors='coerce', yearfirst=True).dt.date\n",
    "# Cambiamos el nombre de la columna 'Periodo' a 'date' para que sea consistente con las fechas de precipitaciones\n",
    "banco = banco.rename({'Periodo': 'date'}, axis=1)\n",
    "# Seteamos el datetime como el index de la tabla\n",
    "banco = banco.set_index('date')\n",
    "# Eliminamos filas con index inválido (Nan)\n",
    "banco = banco[banco.index.notnull()]\n",
    "# Hay datos duplicados, los eliminamos\n",
    "banco = banco.drop_duplicates()\n",
    "# Por último convertimos los valores de las céldas a numérico. \n",
    "# Se puede apreciar que los puntos son usados para indicar miles y no decimales, lo que no es interpretado por Pandas.\n",
    "# Simplemente removemos los puntos y convertimos a numérico. Usamos 'coerce' para forzar inválidos a 'Nan'\n",
    "for col_name in banco:\n",
    "    banco[col_name] = [x.replace('.','') if isinstance(x, str) else x for x in banco[col_name] ]    \n",
    "    banco[col_name] = pd.to_numeric(banco[col_name] ,errors='coerce')\n",
    "# Removemos posibles outliers, es decir datos bajo el cuantil 0.01 y sobre 0.99\n",
    "banco = banco[(banco < banco.quantile(0.99)) & (banco >  banco.quantile(0.01))]\n",
    "# Ordenamos por fecha\n",
    "banco = banco.sort_index()\n",
    "\n",
    "# Nuevamente mostramos los datos para verificar que la tabla está con el formato deseado\n",
    "banco"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Visualización\n",
    "\n",
    "Creamos una función para filtrar dats de precipitaciones de una región en el rango de fechas deseado. En Pandas se puede filtrar facilmente por rangos de fechas!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filt_precip(data, region, start_date, end_date):\n",
    "    \"\"\"\n",
    "        data: pd.DataFrame\n",
    "        region: str\n",
    "        start_date: str or datetime.date\n",
    "        end_date: str or datetime.date\n",
    "    \"\"\"\n",
    "    if isinstance(start_date, str):\n",
    "        start_date = pd.to_datetime(start_date).date()\n",
    "    if isinstance(end_date, str):    \n",
    "        end_date = pd.to_datetime(end_date).date()\n",
    "    \n",
    "    # Nos aseguramos que la región sea válida\n",
    "    assert region in data.columns, \"Región inválida.\"\n",
    "    # Nos aseguramos que la fecha de inicio sea válida    \n",
    "    assert start_date in data.index, \"Fecha de inicio inválida.\"\n",
    "    # Nos aseguramos que la fecha de término sea válida    \n",
    "    assert end_date in data.index, \"Fecha de término inválida.\"  \n",
    "    # Nos aseguramos que la fecha de inicio sea menor a la de término\n",
    "    assert start_date < end_date, \"Fecha de inicio es posterior a la de término\"  \n",
    "    # Filtramos entre las fechas dadas\n",
    "    return data[region][(data.index>start_date) & (data.index<end_date)]  \n",
    "\n",
    "# Función para calcular la media móvil con una ventana deseada\n",
    "def moving_average(x, w):\n",
    "    \"\"\"\n",
    "        x: list or np.array\n",
    "        w: int or float\n",
    "    \"\"\"\n",
    "    return np.convolve(x, np.ones(w), 'same') / w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtenemos los datos para la región metropolitana en las fechas indicadas. Claramente se observa la tendencia estacional de aumentar la cantidad de precipitaciones una vez al año. Se puede apreciar como la media móvil ha ido disminuyendo en las ultimas dos décadas..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos variables con rangos de fecha\n",
    "start_date = '2000-01-01'\n",
    "end_date = '2020-01-01'\n",
    "\n",
    "# Filtramos Datos\n",
    "filt_rm = filt_precip(precip, 'Metropolitana_de_Santiago',start_date, end_date)\n",
    "\n",
    "filt_rm.plot(label='precipitaciones')\n",
    "# tambien graficamos la media móvil con una ventana de 10 \n",
    "ma_rm = moving_average(filt_rm, 10)\n",
    "plt.plot(filt_rm.index,ma_rm, label='media móvil')\n",
    "plt.xlabel('Fecha')\n",
    "plt.ylabel('precipitación [mm]')\n",
    "plt.title('Precipitaciones RM entre {}:{}'.format(start_date, end_date))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtramos datos\n",
    "filt_vi =  filt_precip(precip, 'Libertador_Gral__Bernardo_O_Higgins',start_date, end_date)\n",
    "\n",
    "filt_vi.plot(label='precipitaciones')\n",
    "ma_vi = moving_average(filt_vi, 10)\n",
    "plt.plot(filt_vi.index,ma_vi, label='media móvil')\n",
    "plt.xlabel('Fecha')\n",
    "plt.ylabel('precipitación [mm]')\n",
    "plt.title('Precipitaciones VI entre {}:{}'.format(start_date, end_date))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora creamos una función para graficar una lista de años de una región en particular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista con los nombres de cada mes\n",
    "month_names = ['Ene','Feb','Mar','Abr','May','Jun','Jul','Ago','Sep','Oct','Nov','Dic']\n",
    "\n",
    "def plot_region_by_years(data, region, years):\n",
    "    \"\"\"\n",
    "        data: pd.DataFrame\n",
    "        region: str\n",
    "        years: list\n",
    "    \"\"\"\n",
    "    \n",
    "    # Nos aseguramos que la región sea válida\n",
    "    assert region in data.columns, \"Region inválida.\"\n",
    "    \n",
    "    # Filtramos la región de interés\n",
    "    data = data[region]\n",
    "    \n",
    "    # Creamos dict para almacenar data de los años de interés\n",
    "    plot_data = {}\n",
    "    for year in years:\n",
    "        # Cada valor del dict tendrá una lista con el año y el valor\n",
    "        plot_data[year] = []\n",
    "        for curr_date in data.index:\n",
    "            if curr_date.year == int(year):\n",
    "                plot_data[year].append([curr_date.month, data.loc[curr_date]])\n",
    "\n",
    "    # Creamos objeto para graficar\n",
    "    f, ax = plt.subplots()\n",
    "    for year, values in plot_data.items():\n",
    "        # ordenamos por mes los datos de cada año\n",
    "        sorted(values)\n",
    "        # Convertimos índice de cada mes a su nombre\n",
    "        months = [month_names[x[0]-1] for x in values]\n",
    "        values = [x[1] for x in values]\n",
    "        # Graficamos el año correspondiente\n",
    "        ax.plot(months, values, label=year)\n",
    "    ax.legend()\n",
    "    ax.set_ylabel('Precipitaciones [mm]')\n",
    "    ax.set_title('Precipitaciones de {} para los años {}'.format(region, years))\n",
    "    return ax\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se puede apreciar como la cantidad de lluvias ha disminuído sustancialmente a lo largo de los años. Si bien hay datos faltantes de 1982, la tendendencia es clara y de sostenida en el tiempo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_region_by_years(precip, 'Maule',[1982, 1992, 2002, 2012, 2019])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos una función para graficar series históricas de PIB en un determinado rango de fechas. La verdad es que esta función hace lo mismo que la ya creada anteriormente (*filt_precip*), solamente que ahora debe recibir dos campos. Creamos una función similar a la anterior, pero adaptada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filt_pibs(data, pib1, pib2, start_date, end_date):\n",
    "    \"\"\"\n",
    "        data: pd.DataFrame\n",
    "        pibs: list\n",
    "        start_date: str or datetime.date\n",
    "        end_date: str or datetime.date\n",
    "    \"\"\"\n",
    "    if isinstance(start_date, str):\n",
    "        start_date = pd.to_datetime(start_date).date()\n",
    "    if isinstance(end_date, str):    \n",
    "        end_date = pd.to_datetime(end_date).date()\n",
    "    \n",
    "    # Nos aseguramos que la los campos de pib sea válidos\n",
    "    assert pib1 in data.columns , \"Campo inválido.\"\n",
    "    assert pib2 in data.columns, \"Campo inválido.\"    \n",
    "    # Nos aseguramos que la fecha de inicio sea válida    \n",
    "    assert start_date in data.index, \"Fecha de inicio inválida.\"\n",
    "    # Nos aseguramos que la fecha de término sea válida    \n",
    "    assert end_date in data.index, \"Fecha de término inválida.\"  \n",
    "    # Nos aseguramos que la fecha de inicio sea menor a la de término\n",
    "    assert start_date < end_date, \"Fecha de inicio es posterior a la de término\" \n",
    "    \n",
    "    data = data[[pib1, pib2]]\n",
    "    # Filtramos entre las fechas dadas\n",
    "    return data[(data.index>start_date) & (data.index<end_date)]  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graficamos los PIBs pedidos desde el año 2013 en adelante. Puesto que nuestras tablas están ordenadas por fecha, podemos buscar el último índice para tener la fecha más reciente.\n",
    "\n",
    "Se puede ver que, aunque haya datos faltantes, el PIB de servicios financieros ha ido aumentando consistentemente en los últimos años y no presenta variaciones estacionales. Por otra parte, el PIB agropecuario y silvícola tiene notorias alzas en los periodos de verano y fuertes caídas en invierno, por lo que claramente presenta una periodicidad relacionada con las estaciones del año.\n",
    "\n",
    "Se aprecia que desde el año 2013 hasta el 2018 ambos PIBs fueron aumentando en conjunto(si consideramos los periodos altos del agropecuario y silvícola), pero la tendencia no se aprecia desde el 2019 a la fecha."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_pibs = filt_pibs(banco,'PIB_Agropecuario_silvicola',\n",
    "                          'PIB_Servicios_financieros',\n",
    "                          '2013-01-01',\n",
    "                           banco.index[-1])\n",
    "filtered_pibs.plot()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Tratamiento y creación de variables\n",
    "\n",
    "Una forma clásica de evaluar la correlación entre dos variables es midiendo su coeficiente de correlación de Pearson. Esta entrega una matriz cuadrada y simétrica que indica que tan correlacionadas están las variables. \n",
    "Para entrenar cualquier tipo de modelo es necesario tener variables que estén correlacionadas (ya sea de manera positiva o negativa) con el/los valor/es objetivo. En caso de no haber correlación, o de ser muy pequeña, esos datos no serán útiles para ese problema de predicción/clasificación/regresión.\n",
    "\n",
    "Cargamos los datos de la leche y creamos una variable 'date' a partir de el año y el mes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos datos de la leche\n",
    "leche = pd.read_csv('precio_leche.csv')\n",
    "\n",
    "# Ya habíamos creado una variable month_names. Ahora les asignamos un índice en un diccionario\n",
    "month_dict = {v:k for k,v in enumerate(month_names, start=1)}\n",
    "# Mapeamos los nombres de cada mes a índice en la tabla de datos de leche\n",
    "leche['Mes'] = leche['Mes'].map(month_dict)\n",
    "# Creamos datos del día correspondiente a cada fecha\n",
    "leche['Dia'] = [1]*len(leche.index)\n",
    "# Creamos variable date a partir del año, mes y dia\n",
    "leche['date'] = pd.to_datetime(dict(year=leche.Anio,month=leche.Mes,day=leche.Dia)).dt.date\n",
    "# Eliminamos las columnas de 'Anio', 'Mes' y 'Dia' puesto que ya no son necesarias...\n",
    "leche.drop(['Anio'], axis=1, inplace=True)\n",
    "leche.drop(['Mes'], axis=1, inplace=True)\n",
    "leche.drop(['Dia'], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "# Al igual que con las tablas anteriores, convertimos el date a indice\n",
    "leche = leche.set_index('date')\n",
    "# Eliminamos filas vacías y duplicados\n",
    "leche = leche[leche.index.notnull()]\n",
    "leche = leche.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para realizar el merge entre las bases de datos hay un problema! No tienen los mismos rangos de fechas, por lo que creamos una función para agregar las fechas faltantes (y llenarlas con datos vacíos) entre las bases de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def synch_data(data1, data2, data3, fill=np.nan):\n",
    "    \"\"\"\n",
    "        data1: pd.dataFrame\n",
    "        data2: pd.dataFrame\n",
    "        data3: pd.dataFrame\n",
    "        fill: np.nan or int or float\n",
    "    \"\"\"\n",
    "    # Funcion anidada auxiliar para agregar una nueva fila en la fecha indicada\n",
    "    def fill_data(data, idx):\n",
    "        data.loc[idx] = [fill]*len(data.columns)\n",
    "        return data\n",
    "    # Para cada base de datos comparamos las fechas faltantes en las otras y las agregamos.\n",
    "    for d1 in data1.index:\n",
    "        if d1 not in data2.index:\n",
    "            data2 = fill_data(data2, d1)\n",
    "        if d1 not in data3.index:\n",
    "            data3 = fill_data(data3, d1)                    \n",
    "    for d2 in data2.index:\n",
    "        if d2 not in data1.index:\n",
    "            data1 = fill_data(data1, d2) \n",
    "        if d2 not in data3.index:\n",
    "            data3 = fill_data(data3, d2) \n",
    "    for d3 in data3.index:\n",
    "        if d3 not in data1.index:\n",
    "            data1 = fill_data(data1, d3)\n",
    "        if d3 not in data2.index:\n",
    "            data2 = fill_data(data2, d3)\n",
    "    # Devolvemos los valores sincronizados y ordenados por fecha      \n",
    "    return data1.sort_index(), data2.sort_index(), data3.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Tamaños de las bases de datos antes de sincronizar:\\n  -precipitaciones:{}\\n  -banco:{}\\n  -leche:{}'.format(len(precip.index),len(banco.index),len(leche.index)))\n",
    "precip, banco, leche = synch_data(precip, banco, leche)\n",
    "print('Tamaños de las bases de datos después de sincronizar:\\n  -precipitaciones:{}\\n  -banco:{}\\n  -leche:{}'.format(len(precip.index),len(banco.index),len(leche.index)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora que las bases de datos tienen las mismas fechas, podemos crear una base de datos que sea un *merge* de todas las bases de datos. Como sincronizamos los datos, ahora hay varias filas que contienen principalmente *Nans*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_db = pd.concat([banco, precip, leche], axis=1)\n",
    "merged_db.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Modelo\n",
    "\n",
    "La idea es entrenar un modelo que prediga el valor futuro del precio de la leche en función de sus valores pasados y de valores pasados de otras variables externas. \n",
    "\n",
    "Para este tipo de problema hay varias alternativas, desde análisis clásico de series de tiempo basados en modelos autoregresivos hasta técnicas modernas de *deep learning*, por ejemplo, basadas en redes neuronales recurrentes. Dado que no se cuenta con un volumen tan grande de datos y que varios de estos están incompletos, la mejor alternativa es optar por un método sencillo.\n",
    "\n",
    "Antes de elegir el método, investiguemos el comportamiento del precio de la leche:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtenemos un np.array con el precio de la leche\n",
    "p_leche = merged_db['Precio_leche'].to_numpy()\n",
    "# Indices de fecha\n",
    "idx = merged_db.index\n",
    "\n",
    "plt.plot(idx, p_leche, label='Precio')\n",
    "ma_leche = moving_average(p_leche, 10)\n",
    "plt.plot(idx, ma_leche, label='media móvil')\n",
    "plt.title('Precio de la leche')\n",
    "plt.ylabel('Precio [$]')\n",
    "plt.xlabel('Date')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se puede apreciar que no es estacionario (ni si quiera en el sentido amplio) y que su valor ha ido aumentando con los años. Veamos que pasa con las diferencias de precio entre un año y el anterior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para reemplazar Nans por la interpolación lineal de los valores vecinos que sean válidos \n",
    "def interpolate_nans(y):\n",
    "    def nan_helper(yy):\n",
    "        return np.isnan(yy), lambda z: z.nonzero()[0]\n",
    "    nans, x= nan_helper(y)\n",
    "    y[nans]= np.interp(x(nans), x(~nans), y[~nans])\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interpolamos los valores de los Nans\n",
    "p_leche = interpolate_nans(p_leche)\n",
    "# Obtenemos la derivada de los valores de la leche\n",
    "diff_p_leche = p_leche[1:]-p_leche[:-1]\n",
    "\n",
    "plt.plot(idx[1:],diff_p_leche, label='Precio')\n",
    "ma_leche = moving_average(diff_p_leche, 10)\n",
    "plt.plot(idx[1:], ma_leche, label='media móvil')\n",
    "plt.title('Derivada de precios de la leche')\n",
    "plt.ylabel('Precio [$]')\n",
    "plt.xlabel('Date')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que la diferencia de valores del precio de la leche es estacionario!\n",
    "\n",
    "\n",
    "Por lo tanto usaremos un modelo auto regresivo integrado de promedio movil con variables exógenas (ARIMAX), en que el orden de la parte integrativa será 1. Las variables exógenas deben ser algunos datos del banco central y de las precipitaciones. Para elegir las variables externas, analizaremos los coeficientes de correlación de Pearson entre las variables y el precio de la leche, las que correlacionen mejor serán utilizadas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nombres de las variables que no sean precio se la leche\n",
    "var_names = merged_db.columns.drop('Precio_leche')\n",
    "# Creamos diccionario que almacenará el coeficiente de correlación de cada variable con el precio de la leche:\n",
    "var_corr = {i:None for i in var_names}\n",
    "\n",
    "# iteramos sobre cada variable y calculamos la correlacion\n",
    "for var_name in var_names:\n",
    "    # Extraemos variable y pasamos a np.array\n",
    "    curr_var = merged_db[var_name].to_numpy()\n",
    "    # Inteprolamos Nans\n",
    "    curr_var = interpolate_nans(curr_var)\n",
    "    # Calculamos matriz de correlación. Matriz simétrica, las diagonales son varianzas normalizadas (no interesan)\n",
    "    corr = np.corrcoef(curr_var, p_leche)[0][1]\n",
    "    # Almacenamos en nuestro dict el valor absoluto (correlaciones negativas también sirven!)\n",
    "    var_corr[var_name] = abs(corr)\n",
    "    \n",
    "# Hacemos un rank de las variables según el valor absoluto de la correlación\n",
    "ranked_corrs = {k: v for k, v in sorted(var_corr.items(), key=lambda item: item[1])}\n",
    "ranked_corrs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los valores que tienen mayor correlación (positiva o negativa) son 'Precio_de_la_onza_troy_de_oro_dolaresoz', 'Precio_del_cobre_refinado_BML_dolareslibra' y 'Precio_de_la_onza_troy_de_plata_dolaresoz'!\n",
    "\n",
    "Usaremos estas variables para entrenar el modelo ARIMAX. Utilizaremos la librería *statsmodels* para realizar el entrenamiento y en particular la clase SARIMAX, que es una versión del modelo ARIMAX."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos librerías\n",
    "import random \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "\n",
    "#Number of points\n",
    "n_points = len(merged_db.index)\n",
    "# Agregamos una dimension al precio de la leche\n",
    "p_leche = p_leche.reshape(n_points,1)\n",
    "# Obtenemos nuestras variables exógenas:\n",
    "ex_data1 = interpolate_nans(merged_db['Precio_de_la_onza_troy_de_plata_dolaresoz'].to_numpy()).reshape(n_points,1)\n",
    "ex_data2 = interpolate_nans(merged_db['Precio_del_cobre_refinado_BML_dolareslibra'].to_numpy()).reshape(n_points,1)\n",
    "ex_data3 = interpolate_nans(merged_db['Precio_de_la_onza_troy_de_oro_dolaresoz'].to_numpy()).reshape(n_points,1)\n",
    "ex_data = np.concatenate((ex_data1, ex_data2, ex_data3), axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora debemos ver la forma de separar los datos para poder testear el modelo.\n",
    "Los parámetros del modelo son mejor *fiteados* cuando usamos más datos, sin embargo estamos utilizando un modelo autoregresivo de orden 1. Esto implica que podemos tomar segmentos de tamaño fijo e intentar predecir el siguiente valor del precio de la leche, en base a eso obtener métricas de desempeño.\n",
    "\n",
    "Realizaremos cortes aleatorios de segmentos de 100 valores temporales y mediremos la calidad de la predicción utilizando el error absoluto medio (MAE). Repetiremos este proceso 100 veces, por lo que estaremos entrenando 100 modelos en total! El proceso es rápido, por lo que no debiese demorar mucho.\n",
    "\n",
    "Nota: Aproximadamente los primeros 100 valores del precio de la leche fueron interpolados, asi que no es muy informativo tratar de usarlos. Samplearemos índices desde el 100 en adelante\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtenemos los índices de inicio de las sub-secuencias que muestrearemos\n",
    "random.seed(1)\n",
    "idx = random.sample(range(100, n_points-102), k=100)\n",
    "\n",
    "# Lista de valores objetivo reales\n",
    "targets = []\n",
    "# lista de valores predichos\n",
    "predictions = []\n",
    "\n",
    "for i in idx:\n",
    "    curr_p_leche = p_leche[i:i+100]\n",
    "    curr_ex_data = ex_data[i:i+100,:]\n",
    "    \n",
    "    model = SARIMAX(curr_p_leche, exog=curr_ex_data, order=(1, 1, 1), seasonal_order=(0, 0, 0, 0))\n",
    "    fitted_model = model.fit()\n",
    "    pred = fitted_model.forecast(exog=ex_data[i+10,:])    \n",
    "    \n",
    "    targets.append(p_leche[i+101])\n",
    "    predictions.append(pred)\n",
    "\n",
    "mae = mean_absolute_error(targets, predictions)   \n",
    "print('El MAE de nuestro modelo es:', mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(targets,label='Objetivo')\n",
    "plt.plot(predictions,label='predicción')\n",
    "plt.legend()\n",
    "plt.xlabel('Unidades temporales')\n",
    "plt.ylabel('Precio de la leche')\n",
    "plt.title('Predicción del precio de la leche, MAE:{0:.2f}'.format(mae))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El MAE de la predicción es 6.32, lo cual es bueno. Entre los datos proporcionados hay variables que correlacionan bastante bien con el precio de la leche y se puede apreciar en el gráfico que los valores suelen ser muy similares. Se podría investigar cómo varía el desempeño del modelo agregando otras variables exógenas que correlacionen bien con el precio de la leche, pero habría que investigarlo.\n",
    "\n",
    "Sería bueno tener los datos faltantes, que actualmente fueron interpolados linealmente, para poder aumentar el volumen de datos y tener predicciones más fidedignas. \n",
    "\n",
    "Este tipo de problemas de predicción utilizando series de tiempo se evalúan midiendo el error entre el valor predicho y el real. En nuestro caso consideramos el valor absoluto medio, pero también se podría utilizar el error cuadrático medio (MSE) o su raíz cuadrada (RMSE).\n",
    "\n",
    "Este tipo de modelos predictivos son muy útiles cuando se tiene series de tiempo que correlacionen bien con algun valor a predecir y si existen suficientes datos, se pueden armar modelos más sofisticados que son capaces de obtener muy buenos resultados. En particular se pueden utilizar para predecir características metereológicas, como la cantidad de lluvia que caerá o cuánto sol habrá cada día. Con este tipo de modelos se puede optimizar, por ejemplo, la generación de energías no renovables (si predecimos la potencia del viento o la intensidad del sol, sabemos cuánta energía renovable tendremos) y así no desperdiciar ni contaminar de manera innecesaria. Similarmente, se puede utilizar para tomar decisiones en la agricultura y hacer un uso más eficiente de las aguas o decidir que tipo de árboles plantar para las siguientes temporadas. Estas aplicaciones tienen un efecto directo en el cambio climático, puesto que ayudarían a disminuir contaminación y emisiones no necesarias.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
